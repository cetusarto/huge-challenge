# Huge interview Challenge

My name is César Vega. This is the solution that I came up with for the respective challenge which can be seen in the repository as a pdf.
![enter image description here](https://camo.githubusercontent.com/9940a201ebe79702bc1e10297ad7f2a404d63ae1d5418b0a13fb0e96728fa202/68747470733a2f2f63646e2e737562737461636b2e636f6d2f696d6167652f66657463682f775f313336302c635f6c696d69742c665f6175746f2c715f6175746f3a626573742c666c5f70726f67726573736976653a73746565702f68747470732533412532462532466275636b65746565722d65303562626338342d626161332d343337652d393531382d6164623332626537373938342e73332e616d617a6f6e6177732e636f6d2532467075626c6963253246696d6167657325324666643064373935332d356139642d343431632d623539662d3463646532343435303361315f393334783436312e706e67)

## Environment
The respective python files are expected to run in an environment with the following:

 - Python 3+ installed
 - GCP SDK installed
 - GCP account with project created and respective APIs enabled (needed for Big Query)
 - Access to both public APIs used ([GeoCodeMaps](https://geocode.maps.co/) and [Open-Meteo](https://open-meteo.com/))
 - The following Python modules:
	 - requests
	 - pandas
	 - google-cloud-bigquery
	 - pandas-gbq
	 - sqlite3
 - All local files will be created and modified in the same folder

![enter image description here](https://www.freecodecamp.org/news/content/images/2022/02/Banner-10.png)

## Additional files
### `cities.txt`
Contains a list of cities and their respective country in the following format, one for each line.
> [city],[country]

It can be used to create citiesLoc.txt,
### `citiesLoc.txt`
Same as the previous file with their respective latitude and longitude in the following format.
> [city],[country] [latitude] [longitude]

This file feeds the ETL`.py` fil

### `weather_database.db`
Autogenerated by the `ETL.py` and contains local data for the process and it is used by sqlite

## Python scripts

For the process itself, 3 python scripts are used for different purposes. Theese are:

### `CitiesGeoLoc.py` (Additional script)
This script writes the `citiesLoc.txt` based on the `cities.txt` as default. Uses another API ([GeoCodeMaps](https://geocode.maps.co/) ) which gets the latitude and longitude of each city. It is separated as it is not stated in the challenge information how to get the cities location.


### `ETL.py` 
This script is the main execution. Iterates through the cities found in `citiesLoc.txt` through a process containing the 3 main components:

- Extraction: For a city, uses the API [Open-Meteo](https://open-meteo.com/) and extracts the relevant information for further processing (measurements). To the final Json a timestamp is added (for more acurracy) and the city name too.
- Transformation: For a json, does the respective transformations in pandas (rounding values and using arithmetic formulas to create all needed columns). Uses a helper method to validate the data by:
	- Looking for null values
	- Check the time format
	- Check the expected types and column names for each measurement
- Loading: with the sqlite3 module, executes a ddl to ensure data types before finally loading the processed data in a dataframe. It is worth noting that if the same forecast measures and times are appended, the measurement timestamp will avoid any real duplicates.

For the extraction the requests module was used, due to the use of a public online API. For the processing, pandas dataframe was enough as the needed transformations are pretty light with the data being small enough (240 rows and 8 columns per city ) to not have benefits of parallel processing like when using Spark. Finally the pandas dataframe allows to pretty easily insert the data appending each time as rows while ensuring consistency using the DDL. These are saved in the local database until their loading to Big Query.

**Note:** If the input volume was higher, different frameworks could be used or even a multithread strategy for the extraction and transformation step, but it would be bottlenecked by the sqlite insertion due to it's lightweight engine.

![enter image description here](https://www.tatvasoft.com/blog/wp-content/uploads/2015/07/etl-process-extract-transform-load-1.jpg)
 ### `BQUpload.py`
 This script works with the local database (db file), and using the pandas_gbq module, loads a dataframe into BQ. This is done by:
 1. Checking that the dataset exists, if not creates it
 2. Connects to the Big Query client
 3. Using pandas and sqlite3 extracts the weather_data table and saves it into a dataframe
 4. Using de pandas_gbq module uploads the previous dataframe
 5. Deletes the rows inside the local table to clean space after it is uploaded and secure in Big Query
 6.  (optional) Using an execution argument (either c, k or f for temperature unit) queries Big Query and counts and prints the total rows (10 for each city)

It is worth noting that it is assumed that each prediction is different and that for the same time and city a different measurement timestamp makes these rows different but are still uploaded into the data warehouse. This is taken into account for the Final SQL query as it only aggregates using the latest measurement for each time and city as the following code shows:
```SQL
WITH CTE AS ( (
	SELECT city, CAST(time  AS  DATE) AS  day, temperature_c as temp
	FROM huge-challenge.weather.predictions a 
	WHERE a.measure_ts = (
		SELECT  MAX(b.measure_ts)
		FROM huge-challenge.weather.predictions b
		WHERE b.city = a.city AND b.time = a.time) ) )

SELECT city, DAY, AVG(temp) AS avg_temp, MAX(temp) AS min_temp, MIN(temp) AS max_temp
FROM CTE GROUP BY CITY, DAY ;
``` 
 ![enter image description here](https://cxl.com/wp-content/uploads/2019/10/google-bigquery-logo-1.png)
 

## Conclusion
It was an interesting challenge to work with, it made me put some thought in every part. Even as the final solution is simple, I took every decission based on how efficient it would be with the current requirements and using each tool with their best methodologies. The code has been made with the thought of scaling it, by separating and organizing as much responsabilities as possible for easier maintenance and scalability.
Thank you for the challenge, I had fun working on it :)
